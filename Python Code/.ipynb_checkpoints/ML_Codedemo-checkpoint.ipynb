{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d308e757",
   "metadata": {},
   "source": [
    "# K-Means clustering\n",
    "\n",
    "Clustering algorithms such as K-Means are unsupervised, so they are trained on features/attributes only.\n",
    "The goal is to summarise and find patterns or an underlying structure by organising data into similarity groups (clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e8b77",
   "metadata": {},
   "source": [
    "# Install and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c60609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/loucap/Documents/GitWork/ML_Workshop/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdd9fd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kneed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7d27a2bc9905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# sklearn stands for scikit-learn - a machine learning library: used to build machine learning models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkneed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKneeLocator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# kneed is a package that includes functions to help identify the knee/elbow point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kneed'"
     ]
    }
   ],
   "source": [
    "# Import necessary packages...\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# includes useful functions for manipulating data \n",
    "\n",
    "import numpy as np\n",
    "# includes useful functions for performing mathematical operations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# used for creating visualisations and graphs\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# sklearn stands for scikit-learn - a machine learning library: used to build machine learning models\n",
    "\n",
    "from kneed import KneeLocator\n",
    "# kneed is a package that includes functions to help identify the knee/elbow point \n",
    "\n",
    "# Load up the Iris dataset\n",
    "\n",
    "iris = pd.read_csv(\"Data/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bf538",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b700306",
   "metadata": {},
   "source": [
    "As previously mentioned, clustering algorithms do not require labelled data. But, you'll notice we have a column 'variety' with our target attribute. This can be ignored, as we won't be feeding this target attribute to the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()\n",
    "# allows us to view the first 5 rows of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecec31b",
   "metadata": {},
   "source": [
    "First, let's separate our features from the target attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.iloc[:, [0,1,2,3]].values\n",
    "# iloc helps us select specific rows or columns from a dataset\n",
    "# values returns a NumPy representation of a dataframe - with the axes labels removed\n",
    "# values are stored in a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac642731",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.info()\n",
    "# gives us a concise summary of the dataframe \n",
    "# number of columns, missing data count, data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac90390",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.describe()\n",
    "# tells us about the statistical variation in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e612c93",
   "metadata": {},
   "source": [
    "# Standardise feature data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502e6eb",
   "metadata": {},
   "source": [
    "## Why bother?\n",
    "\n",
    "When it comes to performing clustering on a given dataset, you must think carefully about your feature variables. \n",
    "\n",
    "Perhaps your dataset contains information on bank loans and customer data. It could contain a variable 'annual income' which ranges from £19,000 - £1,000,000, and another variable 'monthly debt' which ranges from £0 - £400,000. Therefore, we must change the values of these numeric columns in the dataset to ensure that they use a common scale, i.e., we standardise/normalise the data. In machine learning, this is referred to as 'feature scaling', and is especially important for clustering given that it is a distance-based algorithm.\n",
    "\n",
    "Because k-means uses the Euclidean distance to calculate the distance between data points and the centroids, we need to ensure that the distance measure accords equal weight to each variable. We don't want to put more weight on variables that might have higher variance. \n",
    "\n",
    "\n",
    "To do this, we will use scikit-learn's preprocessing package which comes with a StandardScaler() class, which is a quick way to perform feature scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "x = ss.fit_transform(x)\n",
    "\n",
    "x = pd.DataFrame(x, columns = iris.columns[:-1])\n",
    "# makes the output easier to read than a big array\n",
    "\n",
    "x\n",
    "# prints dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.describe()\n",
    "# tells us about the statistical variation - we can see how it's changed after standardisation \n",
    "# check the mean and std of each feature\n",
    "# can see mean is very close to 0 and std very close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the frequency distribution of species\n",
    "iris.variety.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b34be",
   "metadata": {},
   "source": [
    "# Let's give clustering a go!\n",
    "\n",
    "Of course, we already know that the optimum number of clusters for classification is going to be 3 (given we have 3 species), but for now, let's pretend that we don't know this and randomly set k = 5.\n",
    "\n",
    "To cluster our data we an use the KMeans class which comes with the scikit-learn package. It has the following parameters:\n",
    "\n",
    "* init - this is the method for initialisation. The standard version of the k-means algorithm is implemented by setting init to \"random\".\n",
    "* n_clusters - this is the number of clusters that you want the algorithm to form, as well as the number of centroids to generate\n",
    "* n_iter - this refers to the number of iterations, i.e., the number of times that the k-means algorithm will be run. This is important because 2 runs can converge on different cluster assignments. The default behaviour for the scikit-learn algorithm is to perform ten k-means runs and then return the results of the one with the lowest sum of the squared error (SSE).\n",
    "* max_iter - this refers to the max number of iterations of the algorithm for a single run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e374f",
   "metadata": {},
   "source": [
    "## Create instance of KMeans class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15daec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=5,\n",
    "    n_init = 10,\n",
    "    max_iter=300,\n",
    "    random_state=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c990b",
   "metadata": {},
   "source": [
    "## Fit K-Means with our features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449a26d",
   "metadata": {},
   "source": [
    "The fit function below comprises the training part of the modelling process. We fit our k-means estimator with our scaled_features. The code below will perform 10 runs of the k-means algorithm on the data, with a maximum of 300 iterations per run - as specified when we created the instance of KMeans class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496091b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d9784",
   "metadata": {},
   "source": [
    "## Access lowest SSE value from the 10 initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_\n",
    "\n",
    "# The lowest SSE value is also referred to as the 'inertia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602151f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_\n",
    "\n",
    "# We can also access the final locations of the centroids\n",
    "# I.e., these are the coordinates of the cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2f302",
   "metadata": {},
   "source": [
    "## What other attributes does KMeans have?\n",
    "\n",
    "You can also get information on the following:\n",
    "\n",
    "* labels_ = labels of each data point\n",
    "* n_iter_ = the number of iterations it took before the algorithm converged\n",
    "* n_features_in_ = the number of features seen during the fit process\n",
    "* feature_names_in_ = the names of features seen during fit process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.n_iter_\n",
    "\n",
    "# It took 13 iterations before the algorithm converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ceff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_\n",
    "\n",
    "# Shows which cluster label each data point has\n",
    "# Remember: in computer science we count from 0, so we do indeed have 5 clusters\n",
    "# These labels are stored as a one-dimensional NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c6cc3",
   "metadata": {},
   "source": [
    "# Visualisation for K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(x.iloc[:,0], x.iloc[:,1], marker = \"o\", c = kmeans.labels_, s = 25, edgecolor = \"k\")\n",
    "# x.iloc[:,0] selects our first feature (sepal.length)\n",
    "# x.iloc[:,1] selects our second featuee (sepal.width)\n",
    "# c - determines the marker colours: takes a sequence of n numbers to be mapped to colours\n",
    "# marker - marker style, whether it's a circle, triangle, square etc\n",
    "# edgecolor - the edge colour of the marker\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c = 'red', marker = '*')\n",
    "# here we're accessing the kmeans attribute 'cluster_centers' and getting coordinates for the first and second feature\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f688b5",
   "metadata": {},
   "source": [
    "# Elbow Method - let's find the optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98780c4",
   "metadata": {},
   "source": [
    "## A brief recap!\n",
    "\n",
    "Determining the number of clusters is a crucial step in the k-means algorithm. You'll notice that each time you increment the number of clusters, the inertia value (the lowest SSE value found during the k-means runs) decreases. That's a good thing right? Not entirely, as at some point this becomes 'over-fitting', which describes a statistical model that fits exactly against its training data. When this happens our model becomes ineffective, as it cannot be used to effectively predict the classes of unseen data. \n",
    "\n",
    "As more centroids are added, the distance from each point to its closest centroid will decrease. So, how do we know when to stop adding centroids? How can we avoid 'over-fitting'?\n",
    "\n",
    "A common method used to evaluate the appropriate number of clusters is the 'elbow method'. This involves running k-means clustering on the dataset for a range of values for k (e.g. 1-10). Then, we compute the SSE values for each k. The elbow method reveals a 'sweet spot' where the SSE curve starts to bend, i.e, its elbow point. This is the point at which diminishing returns are no longer worth the additional cost, i.e., choose the number of clusters so that adding another one doesn't produce a significantly better modelling of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of keyword arguments\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 32}\n",
    "\n",
    "\n",
    "\n",
    "# Here we create a variable 'sse' which contains an empty list\n",
    "\n",
    "sse = []\n",
    "\n",
    "# We then iterate through each k value, ranging from 1-10\n",
    "# Note: range() method does not include the end number in the result, that's why we do 1-11\n",
    "for k in range(2, 11):\n",
    "    \n",
    "#     For each k, we instantiate the kmeans class \n",
    "# Here, the double asterix operator **  is used to unpack a dictionary of keyword arguments, kwargs\n",
    "# We use it to unpack kmeans_kwargs, which have passed to the KMeans function\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    print(kmeans)\n",
    "    kmeans.fit(x)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    \n",
    "# Let's take a look at our SSE scores\n",
    "print(\"\")\n",
    "print(sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ced21",
   "metadata": {},
   "source": [
    "Just from looking at these numbers we can see that after 114.412.. we notice that the inertia starts decreasing in a more linear fashion. This indicates that k=3 will be the optimum number. But plotting this makes this more obvious, as we can observe the bend, so let's go ahead and do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553be7f",
   "metadata": {},
   "source": [
    "# Visualisation for Elbow Method\n",
    "\n",
    "We want to pick the value of k at the \"elbow\" i.e., the point after which the inertia starts decreasing in a linear fashion. In order to observe the elbow, we can plot the number of clusters and the corresponding sse values. In the graph below, it's rather subtle, but we can see that the elbow point is at k=3. In which case, we can determine that the appropriate number of clusters for this dataset, is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0aaed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 11), sse)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e81f3",
   "metadata": {},
   "source": [
    "However, if the graph isn't all that clear, and you'd like a more straightforward means of acquiring the elbow point, you can use the 'kneed' Python package. This comes with the KneeLocator() class, which determines the elbow point programmatically. Let's see if it matches our observation that k should = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bcd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kl = KneeLocator(\n",
    "    range(1, 11), Error, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "\n",
    "# We can now access the 'elbow' variable\n",
    "kl.elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25f9d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution plots - histograms\n",
    "\n",
    "sns.FacetGrid(iris,hue=\"variety\",size=3).map(sns.distplot,\"petal.length\").add_legend()\n",
    "sns.FacetGrid(iris,hue=\"variety\",size=3).map(sns.distplot,\"petal.width\").add_legend()\n",
    "sns.FacetGrid(iris,hue=\"variety\",size=3).map(sns.distplot,\"sepal.length\").add_legend()\n",
    "sns.FacetGrid(iris,hue=\"variety\",size=3).map(sns.distplot,\"sepal.width\").add_legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
