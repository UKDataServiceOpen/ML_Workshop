---
title: "Hierarchical_Clustering"
output: html_document
date: "2022-11-01"
author: Nadia Kennar, UKDS
---

## Recap on hierachical clustering 

Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that clusters similar data points into groups called clusters. The endpoint is a hierarchy of clusters and the objects within each cluster are similar to each other.

There are two approaches: 

- Agglomerative: This is a “bottom-up” approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
- Divisive: This is a “top-down” approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.


## Install and Load Packages


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("factoextra")
#install.packages("stats")
#install.packages("janitor")
#install.packages("ClusterR")
#install.packages("cluster")
#install.packages("tidyverse")
#install.packages("dplyr)
#install.packages("dendextend")
#install.packages("circlize")
#install.packages("colorspace")
  
# Loading package
library(ClusterR)
library(cluster)
library(factoextra)
library(stats)
library(janitor)
library(dplyr)
library(tidyverse)  
library(dendextend)
library(circlize)
library(colorspace)

```


## Information on the packages used for clustering 


Hierarchical clustering can be performed on a data matrix using the function *hclust* from the *cluster* package


# The Iris Dataset 

The famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. (from ?iris)


## Load and Prep the Data

```{r}
iris <- datasets::iris
summary(iris)

#Check for missing values
is.null(iris)

# If your data had missing values you would need to remove them from the rows
#iris <- na.omit(iris)
```


# Method 1

## Basic clustering 

We will perform this on just the numeric values, these include sepal length, petal length, sepal width and petal width which can be represented using the code *[, 1;4]* which selects columns 1 - 4. 


```{r}
data <- dist(iris[, 1:4])
hcluster <- hclust(data)
hcluster
```


## Visualise our clusters using a dendogram 

```{r}
# Convert hclust into a dendrogram and plot
hcd <- as.dendrogram(hcluster)
# Define nodePar
nodePar <- list(lab.cex = 0.6, pch = c(20, 19),
                cex = 0.7, col = c("green","yellow"))
plot(hcd,  xlab = "Height", nodePar = nodePar, main = "Cluster dendrogram",
     edgePar = list(col = c("red","blue"), lwd = 2:1), horiz = TRUE)
```






# Method 2 


```{r}
# Remove the categorical variable 
iris2 <- iris[,-5]

#store as a seperate dataframe 
species <- iris[,5]


species_col <- rev(rainbow_hcl(3))[as.numeric(species)]


```



## Plot a SPLOM (Scatterplot Matrix)


```{r}
# Plot a SPLOM:
pairs(iris2, col = species_col,
      lower.panel = NULL,
       cex.labels=2, pch=19, cex = 1.2)

# Add a legend
par(xpd = TRUE)
legend(x = 0.05, y = 0.4, cex = 3,
   legend = as.character(levels(species)),
    fill = unique(species_col))
par(xpd = NA)
```


We can see that the Setosa species are distinctly different from Versicolor and Virginica (they have lower petal length and width). But Versicolor and Virginica cannot easily be separated based on measurements of their sepal and petal width/length.



# Default hierachical clustering method 

The default hierarchical clustering method in hclust is “complete”. We can visualize the result of running it by turning the object to a dendrogram and making several adjustments to the object, such as: changing the labels, coloring the labels based on the real species category, and coloring the branches based on cutting the tree into three clusters.


```{r}
d_iris <- dist(iris2) # method="man" # is a bit better
hc_iris <- hclust(d_iris, method = "complete")
iris_species <- rev(levels(iris[,5]))

dend <- as.dendrogram(hc_iris)
# order it the closest we can to the order of the observations:
dend <- rotate(dend, 1:150)

# Color the branches based on the clusters:
dend <- color_branches(dend, k=3) #, groupLabels=iris_species)

# Manually match the labels, as much as possible, to the real classification of the flowers:
labels_colors(dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(dend)]
   )]


# We shall add the flower type to the labels:
labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                           "(",labels(dend),")", 
                           sep = "")

# We hang the dendrogram a bit:
dend <- hang.dendrogram(dend,hang_height=0.1)
# reduce the size of the labels:
# dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
dend <- set(dend, "labels_cex", 0.5)
# And plot:
par(mar = c(3,3,3,7))
plot(dend, 
     main = "Clustered Iris data set
     (the labels give the true flower species)", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = iris_species, fill = rainbow_hcl(3))
```

These visualizations easily demonstrates how the separation of the hierarchical clustering is very good with the “Setosa” species, but misses in labeling many “Versicolor” species as “Virginica”.

The hanging of the tree also helps to locate extreme observations. For example, we can see that observation “virginica (107)” is not very similar to the Versicolor species, but still, it is among them. Also, “Versicolor (71)” is located too much “within” the group of Virginica flowers.








