---
title: "Hierarchical_Clustering"
output: html_document
date: "2022-11-01"
author: "Nadia Kennar, UKDS , modified by Ben Anderson"
---

## Recap on hierachical clustering 

Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that clusters similar data points into groups called clusters. The endpoint is a hierarchy of clusters and the objects within each cluster are similar to each other.

There are two approaches: 

- Agglomerative: This is a “bottom-up” approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
- Divisive: This is a “top-down” approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.


## Install and Load Packages

```{r}

#install.packages("factoextra")
#install.packages("stats")
#install.packages("janitor")
#install.packages("ClusterR")
#install.packages("cluster")
#install.packages("tidyverse")
#install.packages("dplyr)
#install.packages("dendextend")
#install.packages("circlize")
#install.packages("colorspace")
  
# Loading package
library(ClusterR)
library(cluster)
library(factoextra)
library(stats)
library(janitor)
library(dplyr)
library(tidyverse)  
library(dendextend)
library(circlize)
library(colorspace)

```


### Information on the packages used for clustering 


Hierarchical clustering can be performed on a data matrix using the function *hclust* from the *cluster* package


## The Iris Dataset 

The famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. (from ?iris)


## Load and Prep the Data

```{r}
iris <- datasets::iris
summary(iris)

#Check for missing values
is.null(iris)

# If your data had missing values you would need to remove them from the rows
#iris <- na.omit(iris)
```


## Method 1

### Basic clustering 

We will perform this on just the numeric values, these include sepal length, petal length, sepal width and petal width which can be represented using the code *[, 1;4]* which selects columns 1 - 4. 


```{r runBasicHcluster}
data <- stats::dist(iris[, 1:4]) # distance matrix , default method = "euclidean"
hcluster <- stats::hclust(data)
hcluster
```


### Visualise our clusters using a dendogram 

```{r dendogramBasic}
# Convert hclust into a dendrogram and plot
hcd <- stats::as.dendrogram(hcluster)
# Define nodePar
nodePar <- list(lab.cex = 0.6, pch = c(20, 19),
                cex = 0.7, col = c("green","yellow"))
plot(hcd,  xlab = "Height", nodePar = nodePar, main = "Cluster dendrogram",
     edgePar = list(col = c("red","blue"), lwd = 2:1), horiz = TRUE)
```






## Method 2 

> This time remove categorical vars and 

```{r prepDataTwo}
# Remove the categorical variable 
iris2 <- iris[,-5]

#store as a separate dataframe 
species <- iris[,5]

#convert to numeric and add a colour palette
species_col <- rev(rainbow_hcl(3))[as.numeric(species)]


```



### Plot a SPLOM (Scatterplot Matrix)

> just to have a look...

```{r pairsPLot}
# Plot a SPLOM:
pairs(iris2, col = species_col,
      lower.panel = NULL,
       cex.labels=2, pch=19, cex = 1.2)

# Add a legend
par(xpd = TRUE)
legend(x = 0.05, y = 0.4, cex = 3,
   legend = as.character(levels(species)),
    fill = unique(species_col))
par(xpd = NA)
```


We can see that the Setosa species are distinctly different from Versicolor and Virginica (they have lower petal length and width). But Versicolor and Virginica cannot easily be separated based on measurements of their sepal and petal width/length.



### Default hierachical clustering method 

The default hierarchical clustering method in hclust is “complete”. We can visualize the result of running it by turning the object to a dendrogram and making several adjustments to the object, such as: changing the labels, coloring the labels based on the real species category, and coloring the branches based on cutting the tree into three clusters.

> try method = "single", method = "complete", method = "average"

 * Single works poorly
 * Complete works OK (probably best)
 * Average = all pairwise similarities & uses averages as distance

```{r hclustComplex}
d_iris <- stats::dist(iris2) # method="man" # is a bit better
hc_iris <- stats::hclust(d_iris, method = "average")
iris_species <- rev(levels(iris[,5]))

dend <- stats::as.dendrogram(hc_iris)
# order it the closest we can to the order of the observations:
dend <- rotate(dend, 1:150)

# Color the branches based on the clusters:
dend <- dendextend::color_branches(dend, k=3) #, groupLabels=iris_species)

# Manually match the labels, as much as possible, to the real classification of the flowers:
labels_colors(dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(dend)]
   )]


# Add the flower type to the labels:
labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                           "(",labels(dend),")", 
                           sep = "")

# We hang the dendrogram a bit:
dend <- dendextend::hang.dendrogram(dend,hang_height=0.1)
# reduce the size of the labels:
# dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
dend <- set(dend, "labels_cex", 0.2)
# And plot:
par(mar = c(3,3,3,7))
plot(dend, 
     main = "Clustered Iris data set", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = iris_species, fill = rainbow_hcl(3))
```

These visualizations easily demonstrates how the separation of the hierarchical clustering is very good with the “Setosa” species, but misses in labeling many “Versicolor” species as “Virginica”.

The hanging of the tree also helps to locate extreme observations. For example, we can see that observation “virginica (107)” is not very similar to the Versicolor species, but still, it is among them. Also, “Versicolor (71)” is located too much “within” the group of Virginica flowers.

> Measures of 'goodness'?

### Elbow plot method

Similar to how we determined optimal clusters with k-means clustering, we can execute similar approaches for hierarchical clustering:

Elbow Method: To perform the elbow method we just need to change the second argument in fviz_nbclust to FUN = hcut.

```{r lookForElbow}
factoextra::fviz_nbclust(iris, FUN = hcut, method = "wss")

```




### Examining different clustering methods (performance)

> test performance of "single" vs "complete" vs "average"

> XX added other linkage options to test

```{r comparingMethods}
# Select the methods of interest
hclust_methods <- c("single", "complete", "average", 
                    "ward.D", "ward.D2", "centroid",
                    "mcquitty","median")
iris_dendlist <- dendextend::dendlist()
for(i in seq_along(hclust_methods)) {
   hc_iris <- stats::hclust(d_iris, method = hclust_methods[i])   
   iris_dendlist <- dendextend::dendlist(iris_dendlist,
                                         as.dendrogram(hc_iris))
}
names(iris_dendlist) <- hclust_methods
iris_dendlist


## Obtain coefficients and plot 
iris_dendlist_cor <- dendextend::cor.dendlist(iris_dendlist)
iris_dendlist_cor
corrplot::corrplot(iris_dendlist_cor, "pie", "lower")


```

> XX so they all correlate except 'complete' which suggests 'complete' is best (why?)

> XX Doesn't this just show which produce the same results? YES

> Also try ward.D as a linkage method?


#### References: 
1) https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html
2) https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html#iris---edgar-andersons-iris-data
2) https://www.kaggle.com/code/anna1231/hierarchical-clustering-of-iris-species/notebook 







