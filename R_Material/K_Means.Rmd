---
title: "K_Means"
output: html_document
date: "2022-11-01"
author: Nadia Kennar, UKDS
---

## Recap on K-Means 

Clustering is a technique in machine learning that attempts to find clusters of observations within a dataset.The goal is to find clusters such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.
Clustering is a form of unsupervised learning because we’re simply attempting to find structure within a dataset rather than predicting the value of some response variable.

K - Means: 

K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.
Typically, unsupervised algorithms make inferences from datasets using only input vectors without referring to known, or labelled, outcomes. K-means clustering is a technique in which we place each observation in a dataset into one of K clusters.

The end goal is to have K clusters in which the observations within each cluster are quite similar to each other while the observations in different clusters are quite different from each other.

1. Choose a value for K
2. Randomly assign each observation to an inital cluster, from 1 to K
3. Compute the cluster centroid (vector of the P feature means for the observation in the Kth cluster)



## Install and Load Packages

```{r setup, include=FALSE, echo=FALSE}

#install.packages("factoextra")
#install.packages("stats")
#install.packages("janitor")
#install.packages("ClusterR")
#install.packages("cluster")
#install.packages("tidyverse")
#install.packages("dplyr)
  
# Loading package
library(ClusterR)
library(cluster)
library(factoextra)
library(stats)
library(janitor)
library(dplyr)
library(tidyverse)  

```



We are going to explore datasets
- 1) USArrests (R Dataset)
- 2) Iris Dataset


## 1 - USA Arrests DataSet (R DataSet)

```{r}
#read in data 
df <- USArrests %>% na.omit(df)

## Scaling and standardising 
df <- scale(df)
head(df)

#compute distance matrix
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


## K-Means clustering 

k2 <- kmeans(df, centers = 2, nstart = 25)
str(k2)
k2

#If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

#visualise 
a <- fviz_cluster(k2, data = df)
a

#Or you could use standard pairwise scatter plots 
df %>%
  as_tibble() %>%
  mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>%
  ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
  geom_text()
```






## 2 - IRIS Dataset 

K-means Clustering is used with unlabeled data, but in this case, we have a labeled dataset so we have to use the iris data without the Species column. In this way, algorithm will cluster the data and we will be able to compare the predicted results with the original results, getting the accuracy of the model.


```{r}
ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point(aes(col=Species), size=4)

```



As we can see, setosa is going to be clustered easier. Meanwhile, there is noise between versicolor and virginica even when they look like perfectly clustered.

Let’s run the model. kmeans is installed in the base package from R, so we don’t have to install any package.

In the kmeans function, it is necessary to set center, which is the number of groups we want to cluster to. In this case, we know this value will be 3. Let’s set that.

, but let’s see how we would build the model if we didn’t know it.


```{r}
set.seed(101)
irisCluster <- kmeans(iris[,1:4], center=3, nstart=20)
irisCluster

#Compare the predicted clusters 
table(irisCluster$cluster, iris$Species)

```


## Plot 

```{r}
clusplot(iris, irisCluster$cluster, color=T, shade=T, labels=0, lines=0)
```


## Evaluation:: the elblow plot method 

It might not always be known the exact number of centers, especially when we have unlabelled datasets. Therefore, we can use the Elblow plot method to examine the centers we defined 


```{r}
tot.withinss <- vector(mode="character", length=10)
for (i in 1:10){
  irisCluster <- kmeans(iris[,1:4], center=i, nstart=20)
  tot.withinss[i] <- irisCluster$tot.withinss
}


plot(1:10, tot.withinss, type="b", pch=19)

```

















