---
title: "Extra Activity"
output: html_document
date: "2022-10-27"
author: Nadia Kennar, UKDS
---




## Install  Packages

```{r setup, include=TRUE, echo=TRUE}

# These installs should be executed in the console

# install.packages("factoextra")
# install.packages("stats")
# install.packages("janitor")
# install.packages("ClusterR")
# install.packages("cluster")
# install.packages("tidyverse")

# Loading packages
library(ClusterR)
library(cluster)
library(factoextra)
library(stats)
library(janitor)
library(dplyr)
library(tidyverse)
library(cluster)


```

# Introduction

**Clustering** is a technique in machine learning that attempts to find clusters of observations within a dataset.

The goal is to find clusters such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.

Clustering is a form of unsupervised learning because we’re simply attempting to find structure within a dataset rather than predicting the value of some response variable.

*K - Means* clustering is a technique in which we place each observation in a dataset into one of K clusters.

In terms of input, the K-Means algorithm requires numeric data, as it is a distance based algorithm that calculates centroids by taking the average of all data points within a cluster. Therefore, K-Means is not a suitable algorithm for categorical data.
However, we can convert categorical data into numeric data. For instance, text data can be converted into numeric data by using a technique called vectorisation, which assigns each word in the text a numeric value. 

The end goal is to have K clusters in which the observations within each cluster are quite similar to each other while the observations in different clusters are quite different from each other. 

How it works:

1. Choose a value for K
2. Randomly assign each observation to an initial cluster, from 1 to K
3. Compute the cluster centroid (vector of the P feature means for the observation in the Kth cluster)
4. For each observation, calculate the Euclidean distance between the observation and the centroid of its assigned cluster
5. Reassign the observation to the cluster with the smallest Euclidean distance
6. Repeat steps 3 to 5 until no further changes are made


We are going to explore two synthetic datasets
- 1) USArrests (R Dataset)
- 2) Wholesale customer data (UCI ML repository)

# Case Studies

## 1 - USA Arrests DataSet (R DataSet)

```{r kMeansExampleArrests}
#read in data 
df <- USArrests %>% na.omit(df)

## Scaling and standardising 
head(df)
df <- scale(df) # centres & standardises data etc - requires numeric-only columns, states are row names
head(df)

#compute distance matrix
distance <- factoextra::get_dist(df)
factoextra::fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


## K-Means clustering 

k2 <- stats::kmeans(df, centers = 2, nstart = 25)
str(k2)
k2

#If there are more than two dimensions (variables) factoextra::fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

#visualise 
arrestsPlot <- factoextra::fviz_cluster(k2, data = df)
arrestsPlot

#Or you could use standard pairwise scatter plots 
df %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>%
  ggplot2::ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
  geom_text()
```

>**Useful tip:**

>If there are more than two dimensions (variables) factoextra::fviz_cluster will perform principal component analysis (PCA) and plot the data points (and the clusters) according to the first two principal components that explain the majority of the variance.

2 - Customer Data 

Now that we've covered the basics of the k-mean algorithm using the open R Dataset, lets explore some real-world data!


The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories. It includes the annual spending on a range of product categories. Let's see if we can use k-means clustering to segment/group these customers based on their spending habits.

```{r read in the data }
#read in data
customer <- read.csv("Wholesale customers data.csv") %>% clean_names()
```

Lets briefly explore the datatset

```{r explore the dataset }
head(customer)
```


### Data Pre-Processing 

*Information about the variables*

Nominal:

- Channel: HORECA i.e. hotel/restaurant/cafe (1) or Retail Channel (2)
- Region: Linson (1), Oporto (2) or Other (3)



Continuous 
- the remaining variables are continuous and represent the annual spending (in monetary units) of each diverse product categories



Let's have a closer look at our variables to see if we need to perform any pre-processing. It's clear that the variables 'Channel' and 'Region' have been label encoded (each value has been converted into a number), therefore the variance for these variables will be extremely low compared to the product variables.

K-Means can be used with nominal variables that have been converted into numbers, even if the variance for these variables is low. However, it is important to keep in mind that low variance variables may not be ideal for K-Means clustering, as they may not provide enough information to accurately identify clusters. This is  because they lack the ability to differentiate between data points. For example, if all the data points in a cluster have the same value for a variable with low variance, then it is impossible to determine which points could be clustered together.

A way of dealing with variables like Channel and Region (which have low magnitudes), is to standardise them so that all variables have the same magnitude. This is what we do later in the 'Standardise the data' section.


### Descriptive Analysis 

```{r}
#Use summary() function to run descriptive analysis
summary(customer)

#Use ftable() function on categorical variables
table(customer$channel)
table(customer$region)
```

When it comes to performing clustering on a given datset, you must think carefully about feature variables. 


Our product variables have different ranges of monetary units i.e. fresh ranges from $3 - £112151 whereas frozen ranges from £25 - £60869. We must change the values of these numeric columns in the dataset to ensure that they use a common scale i.e. we standarise/normalise the data . In machine learning this is referred to as 'feature scaling'. 

This is important for clustering given uses distanced-based algorithm. 

Because k-means uses the Euclidean distance to calculate the distance between data points and the centroids, we need to ensure that the distance measure accords equal weight to each variable. We don't want to put more weight on variables that might have higher variance.



### Standarise the Data


Forcing standardisation on variables with only two or three values can have a negative effect on standardization, as it limits the range of values that can be used for the variables. This can make it more difficult to interpret the results of the K-Means clustering. It is generally recommended to use variables with a wide range of values for standardization, as this can provide more meaningful results. In this demonstration we will be keeping 'channel' and 'region' as they convey important information, but this is why it is useful to explore different clustering algorithms too. Perhaps Hierarchical Clustering would work better for this dataset, given that it can handle both numeric and categorical data.

Also, why not try excluding "channel" and "region", and see how clustering performs then?

```{r standardiseCD}
customer <- scale(customer) # centres & standardises data
head(customer)
```




### Fitting k-means clustering model

We can compute k-means in R with the kmeans function. Here will group the data into two clusters (centers = 3). The kmeans function also has an nstart option that attempts multiple initial configurations and reports on the best one. For example, adding nstart = 20 will generate 20 initial configurations. This approach is often recommended.


As mentioned in the webinar, it is generally recommended to run the algorithm at least 10+ times with different starting points. You will notice in the arrests example we used 25 initial configurations. We recommend that you go ahead and try running KMeans with different numbers of configurations and see what happens! 


```{r}
set.seed(240) # Setting seed
kmeans.re <- kmeans(customer, centers = 3, nstart = 20)
kmeans.re

# here we access the cluster attribute
# cluster is a vector containing the cluster assignments of each observation
kmeans.re$cluster
  
# Confusion Matrix
#cm <- table(customer$region, kmeans.re$cluster)
#cm
```


If we print the results we'll see our groupings results in 3 clusters sizes of 130, 14, 296. We also get the means for the 3 groups across the variables, and the cluster assignment for each observation



### Visualisation 

```{r}
fviz_cluster(kmeans.re, data = customer)
```


The above visualisation promotes doubt that there are really 3 clusters in this dataset. The green cluster seems unlikely, as it encompasses sparse data points that are quite far apart from each other. Let's see what the elbow method uncovers. 



### Evaluation: the elbow method and silhouette method




```{r evalnClustersCD}

#Elbow Method for finding the optimal number of clusters

#Step by Step 
set.seed(123)

# Compute and plot wss for k = 2 to k = 15.
k.max <- 15

wss <- sapply(1:k.max, 
              function(k){kmeans(customer, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")


#Using fviz_nbclust
set.seed(123)


fviz_nbclust(customer, kmeans, method ="wss")

factoextra::fviz_nbclust(customer, kmeans, method = "silhouette")

```
As we can see the lowest SSE result produced from the algorithm runs, is quite high. If you click on the first output image, you will see that the lowest SSE is 674.0171. When we say 'quite high' it should be noted that this depends on the context. Generally, 674.0171 is considered to be relatively high, as it indicates that the observations are far from their cluster centroids. However, it is important to consider the size of the dataset and the number of clusters when interpreting the SSE, as a higher SSE may be expected with larger datasets or more clusters.


This inertia value suggests that our current kmeans model is not a good fit for the data. We want to reduce this number considerably. The inertia value is sensitive to the number of clusters that we use when running the algorithm. 

Therefore, to help find the optimum number of clusters, we can refer to the results of the elbow method, which is the first plot shown above. Here, we can see a definite bend where k = 2, suggesting that our optimal k value is 2! 

The third plot shown, is a silhouette plot, which is another method of evaluating the quality of a K-Means clustering. Whilst the elbow plot shows the total within-clusters sum of squares (wss) for different numbers of clusters (K), the silhouette plot shows the silhouette width for different numbers of clusters. The wss measures the sum of the squared differences between the observations and their cluster centroids, while the silhouette width measures the compactness and separation of the clusters. Both plots can be used to determine the optimal number of clusters for the data, but the silhouette plot is generally preferred, as it provides more information about the quality of the clustering.

In terms of how we interpet a silhouette plot, a silhouette value close to 1 indicates that the data point is well-matched to its own cluster and far from other clusters. Meanwhile, a value close to -1 indicates that the data point is misclassified and may belong to another cluster. We can see here that k = 3 is closest to 1, at nearly 0.4 on the y-axis. 

So, it seems we have an issue:

- Elbow method: 2 cluster solution suggested
- Silhouette method: 3 cluster solution suggested

### Evaluation: the gap statistic method

Another evaluation method that we could try (to help us break this stalemate) is the gap statistic method. 

It is different from the elbow method and silhouette method because it does not rely on visual inspection of the data. Instead, it uses the ratio of the total within-cluster sum of squares to the total sum of squares from the centroid of the entire dataset.

The Gap statistic measures the difference between the sum of squared distances between each data point and its cluster centroid in the clusters found with the chosen number of clusters, and the sum of squared distances between each data point and the centroid of the entire dataset. Thus, it measures the improvement in clustering that is obtained by adding more clusters.

Interpreting the Gap statistic is relatively straightforward: larger values indicate that more clusters are likely to be appropriate for the data. It is important to note, however, that the Gap statistic can be sensitive to outliers and should be interpreted with caution.

Let's go ahead and give it a go. 

```{r evalnClustersCD}

factoextra::fviz_nbclust(customer, kmeans, method = "gap_stat")
```

The plot above indicates that the optimal value for k is k = 2. Therefore, given that 2/3 of our evaluation methods point to the same k value, let's retry the KMeans algorithm with k=2. 


### Retry with k = 2

```{r kmeansCDfinal}
set.seed(240) # Setting seed
kmeansFinal.re <- stats::kmeans(customer, centers = 2, nstart = 25)
kmeansFinal.re



### Extra Activities: PCA on the customer dataset

factoextra::fviz_cluster(kmeansFinal.re, data = customer)

```

We can see that we still get two main clusters in the top right hand corner, with cluster 1 in particular encompassing some data points that are quite far apart. Perhaps you could see what clustering would look like if you excluded some outliers like datapoint 86 and 184. Or you could try clustering this data again with different clustering algorithms to see if you get a better result!


If there are more than two dimensions (variables) factoextra::fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance. We can see that this occurs when we first plotted the arrests dataset, as well as the wholesale customers dataset.

However, now we are going to do some explicit PCA analysis, which will give us a more in-depth understanding of the concept, and why it is useful. To do this, we will be making use of the 'prcomp' function which is available in the stats package.

The goal of PCA is to explain most of the variability in a dataset with fewer variables than the original dataset.



```{r}
#calculate principal components

# scale = TRUE, so that each of variables in dataset is scaled to have a mean of 0 and std of 1 
results <- stats::prcomp(customer, scale = TRUE)

head(results)

#reverse the signs
# we reverse signs of the PCs to ensure they are all in the same direction
# in cases where data is highly correlated the PCA algorithm may end up reversing the signs of some of the components which can lead to confusion
results$rotation <- -1*results$rotation

#display principal components
results$rotation

```

```{r}
#reverse the signs of the scores
results$x <- -1*results$x

#display the first six scores
head(results$x)


# print results
print(results)

```
The code 'results$rotation <- -1*results$rotation' reverses the signs of the principal components, which are linear combinations of the original variables. These principal components are calculated to maximize the variance in the data. The code 'results$x <- -1*results$x' reverses the signs of the scores, which are the projections of the original data onto the principal components. Reversing the signs of the principal components ensures that all of the components are in the same direction, making the results easier to interpret. Reversing the signs of the scores ensures that the data points are projected in the same direction as the principal components, thus maintaining the patterns of variance in the data.

The standard deviations indicate how much variance is explained by each component. The higher the standard deviation, the more variance is explained by that component. Thus, PC1 has the highest standard deviation (1.76) and explains the most variance in the data.

The rotation matrix shows how each of the original variables contributes to each of the components. For example, the first row in the rotation matrix indicates that the “channel” variable has a positive influence on PC1 and a negative influence on PC2.

Overall, this code shows the results of a principal component analysis, which can be used to reduce the dimensionality of a dataset.

Let's have a look at these results in a scree plot.

```{r}
#scree plot results
plot(results)

```
A scree plot is used to determine the optimal number of components to retain in a principal component analysis (PCA). The plot shows the eigenvalues of the principal components on the y-axis and the principal components on the x-axis. Generally, the eigenvalues will decrease as the number of components increases. The “elbow” of the plot is the point where the eigenvalues drop off significantly and indicates the optimal number of components to retain. To interpret the scree plot, look for the “elbow” in the plot and identify the number of principal components that correspond to that point. That number of components is the optimal number to retain for the PCA. In this case, we should retain PC1 and PC2 as they explain the majority of the variance in the dataset.


### Visualise 

```{r}
biplot(results, scale = 0)
```



## Extra Activities: Looking at just frozen vs grocery 

```{r}
#### Model Evaluation and visualization

plot(customer[c("grocery", "frozen")])
plot(customer[c("grocery", "frozen")],
     col = kmeans.re$cluster)
plot(customer[c("grocery", "frozen")],
     col = kmeans.re$cluster,
     main = "K-means with 3 clusters")

## Plotiing cluster centers
kmeans.re$centers
kmeans.re$centers[, c("grocery", "frozen")]

# cex is font size, pch is symbol
points(kmeans.re$centers[, c("grocery", "frozen")],
       col = 1:3, pch = 8, cex = 3)

## Visualizing clusters
y_kmeans <- kmeans.re$cluster
clusplot(customer[, c("grocery", "frozen")],
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste("Cluster"),
         xlab = 'grocery',
         ylab = 'frozen')

```

From the plot we can see each of our customers represented in a simple two-dimensional space.

The customers that are close to each other on the plot have similar data patterns in regards to the variables in the original dataset.

We can also see that certain customers are more highly associated with certain variables than others. For example, customer 66 and 334 are closest to the variable detergents_paper in the plot. 

If we take a look at customers with the highest detergents_paper in the original dataset, we can see that customers 86 and 334 are at the top of the list.

```{r}
customer <- read.csv("Wholesale customers data.csv") %>% janitor::clean_names()
head(customer[order(-customer$detergents_paper),])

```




# The end


```{r runToHere}
```


