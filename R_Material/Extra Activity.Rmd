---
title: "K_means examples (UKDS workshop)"
subtitle: ""
author: "Nadia Kennar, UKDS, modified by Ben Anderson"
output:
  html_document:
    toc: true
    toc_float: true
date: "Last run: `r Sys.time()`"
---

## Code

  * Original code & contributors: https://github.com/UKDataServiceOpen/ML_Workshop
  * This fork: https://github.com/dataknut/ML_Workshop

## Install  Packages

```{r setup, include=TRUE, echo=TRUE}

#install.packages("factoextra")
#install.packages("stats")
#install.packages("janitor")
#install.packages("ClusterR")
#install.packages("cluster")
#install.packages("tidyverse")

# Loading package
# library(ClusterR)
# library(cluster)
# library(factoextra)
# library(stats)
# library(janitor)
# library(dplyr)
# library(tidyverse)  
# library(cluster)

library(dkUtils) # find it at https://github.com/dataknut/dkUtils

myLibs <- c("factoextra", "stats", "janitor", "ClusterR", "cluster", "tidyverse",
            "ggplot2")
dkUtils::loadLibraries(myLibs) # magic
```


# Introduction

**Clustering** is a technique in machine learning that attempts to find clusters of observations within a dataset.

The goal is to find clusters such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.

Clustering is a form of unsupervised learning because we’re simply attempting to find structure within a dataset rather than predicting the value of some response variable.

*K - Means* clustering is a technique in which we place each observation in a dataset into one of K clusters.

> k-means requires numeric data as it wants to compute centroids? (check)

The end goal is to have K clusters in which the observations within each cluster are quite similar to each other while the observations in different clusters are quite different from each other.

How it works:

1. Choose a value for K
2. Randomly assign each observation to an initial cluster, from 1 to K
3. Compute the cluster centroid (vector of the P feature means for the observation in the Kth cluster)


We are going to explore two synthetic datasets
- 1) USArrests (R Dataset)
- 2) Wholesale customer data (UCI ML repository)

# Case Studies

## 1 - USA Arrests DataSet (R DataSet)

> XX why nstart = 25? See notes in [K_Means.Rmd](K_Means.Rmd)
> X why centres = 2? Do we suspect 2 clusters... or...what?

```{r kMeansExampleArrests}
#read in data 
df <- USArrests %>% na.omit(df)

## Scaling and standardising 
head(df)
df <- scale(df) # centres & standardises data etc - requires numeric-only columns, states are row names
head(df)

#compute distance matrix
distance <- factoextra::get_dist(df)
factoextra::fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


## K-Means clustering 

k2 <- stats::kmeans(df, centers = 2, nstart = 25)
str(k2)
k2

#If there are more than two dimensions (variables) factoextra::fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

#visualise 
arrestsPlot <- factoextra::fviz_cluster(k2, data = df)
arrestsPlot

#Or you could use standard pairwise scatter plots 
df %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>%
  ggplot2::ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
  geom_text()
```

>**Useful tip:**

>If there are more than two dimensions (variables) factoextra::fviz_cluster will perform principal component analysis (PCA) and plot the data points (and the clusters) according to the first two principal components that explain the majority of the variance.

2 - Customer Data 

Now that we've covered the basics of the k-mean algorithm using the open R Dataset, lets explore some real-world data!

The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories. It includes the annual spending on a range of product categories. Let's see if we can use k-means clustering to segment/group these customers based on their spending habits.


> janitor::clean_names() - "Resulting names are unique and consist only of the _ character, numbers, and letters"
=======
The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories. It includes the annual spending on a range of product categories. Let's see if we can use k-means clustering to segment/group these customers based on their spending habits.


```{r read in the data}
# read in data
customer <- read.csv("Wholesale customers data.csv") %>% janitor::clean_names()
```

Lets briefly explore the datatset

```{r explore the dataset}
head(customer)
```


### Data Pre-Processing 

*Information about the variables*

Nominal:

- Channel: HORECA i.e. hotel/restaurant/cafe (1) or Retail Channel (2)
- Region: Linson (1), Oporto (2) or Other (3)


Continuous 
- the remaining variables are continuous and represent the annual spending (in monitary units) of each diverse product categories
- the remaining variables are continuous and represent the annual spenaing (in monitary units) of each diverse product categories

Let's have a closer look at our variables to see if we need to perform any pre-processing. It's clear that the variables 'Channel' and 'Region' have been label encoded (each value has been converted into a number), therefore the variance for these variables will be extremely low compared to the product variables.

> XX so can we use nominal variables in k-means in this way??

### Descriptive Analysis 

```{r descriptivesCD}
#Use summary() function to run descriptive analysis
summary(customer)

#Use ftable() function on categorical variables
table(customer$channel)
table(customer$region)
```

When it comes to performing clustering on a given datset, you must think carefully about feature variables. 

Our product variables have different ranges of monitory units i.e. milk ranges from $3 - £112151 whereas frozen ranges from £25 - £60869. We must change the values of these numeric columns in the dataset to ensure that they use a common scale i.e. we standarise/normalise the data . In machine learning this is referred to as 'feature scaling'. 

This is important for k-means clustering as it uses a *distanced-based* algorithm. 

Because k-means uses the Euclidean distance to calculate the distance between data points and the centroids, we need to ensure that the distance measure accords equal weight to each variable. We don't want to put more weight on variables that might have higher variance.

To standardise a dataset we can use `scale` (which was used in the arrests example above as well)


### Standarise the Data

> XX what is effect on standardisation of forcing it to use the channel & region variables (which only have 2 or 3 values)?

```{r standardiseCD}
customer <- scale(customer) # centres & standardises data
head(customer)
```




### Fitting k-means clustering model

We can then compute k-means in R on our scaled data using the stats::kmeans function. Here will start by grouping the data into -> three <- clusters (centers = 3). The kmeans function also has an nstart option that attempts multiple initial configurations and reports on the best one. For example, adding nstart = 20 will generate 20 initial configurations. This approach is often recommended.

> XX why is nstart = 20 used here but 25 used in the arrests example?

```{r kmeansCD}
set.seed(240) # Setting seed
kmeans.re <- stats::kmeans(customer, centers = 3, nstart = 20)
kmeans.re

#cluster identification?
kmeans.re$cluster
  
# Confusion Matrix
#cm <- table(customer$region, kmeans.re$cluster)
#cm
```


If we print the results we'll see our groupings results in 3 clusters sizes of 130, 14, 296. We also get the means for the 3 groups across the variables, and the cluster assignment for each observation


### Visualusisation 

Draw a pretty plot

```{r kmeansVisCD}
factoextra::fviz_cluster(kmeans.re, data = customer)
```

> XX do we really think there ar three clusters?


### Evaluation: the elbow method

As we can see the lowest SSE produced from the algorithm runs, is quite high. 

> XX which result is this?
> And what does 'quite high' mean? Relative to what?

This suggests that our current kmeans model is not a good fit for the data. We want to reduce this inertia value considerably. The inertia value is sensitive to the number of clusters that we use when running the algorithm. Therefore, to help find the optimum number of clusters, let's use the aforementioned elbow method.

> XX aforementioned where?

```{r evalnClustersCD}
#Elbow Method for finding the optimal number of clusters

#Step by Step 
set.seed(123)

# Compute and plot wss for k = 2 to k = 15.
k.max <- 15

wss <- sapply(1:k.max, 
              function(k){kmeans(customer, k, nstart=50,
                                 iter.max = 15 )$tot.withinss}) #extrasts tot.withinss for each loop
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")


#Using fviz_nbclust
set.seed(123)

factoextra::fviz_nbclust(customer, kmeans, method ="wss")

```

> XX So what does that tell us and what should we do next? Should we go with 6 as it's a point of inflection?

### Retry with k = 6

```{r kmeansCDfinal}
set.seed(240) # Setting seed
kmeansFinal.re <- stats::kmeans(customer, centers = 6, nstart = 25)
kmeansFinal.re

#cluster identification?
kmeansFinal.re$cluster
  
# Confusion Matrix
#cm <- table(customer$region, kmeansFinal.re$cluster)
#cm

factoextra::fviz_cluster(kmeansFinal.re, data = customer)
```
> Hmmmmm

### Extra Activities: Principal Component Analysis (PCA) on the customer dataset

If there are more than two dimensions (variables) factoextra::fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

> XX looks like this code just does explicit PCA using `prcomp`

> XX why are we reversing signs etc?

```{r pcaCD}
#calculate principal components
results <- stats::prcomp(customer, scale = TRUE)

#reverse the signs
results$rotation <- -1*results$rotation

#display principal components
results$rotation

#reverse the signs of the scores
results$x <- -1*results$x

#display the first six scores
head(results$x)

# print results
print(results)

#scree plot results
plot(results)
```
> XX what was the point of all that?

### Visualise 

> X - seems to (atttempt to) plot the dimensions with axes

"A biplot is plot which aims to represent both the observations and variables of a matrix of multivariate data on the same plot."

```{r visualisePCA}
stats::biplot(results, scale = 0)
```

> right

## Extra Activities: Looking at just frozen vs grocery 

```{r extraCD}
# #### Model Evaluation and visualization
# 
# plot(customer[c("grocery", "frozen")])
# plot(customer[c("grocery", "frozen")], 
#      col = kmeans.re$cluster)
# plot(customer[c("grocery", "frozen")], 
#      col = kmeans.re$cluster, 
#      main = "K-means with 3 clusters")
#   
# ## Plotiing cluster centers
# kmeans.re$centers
# kmeans.re$centers[, c("grocery", "frozen")]
#   
# # cex is font size, pch is symbol
# points(kmeans.re$centers[, c("grocery", "frozen")], 
#        col = 1:3, pch = 8, cex = 3) 
#   
# ## Visualizing clusters
# y_kmeans <- kmeans.re$cluster
# clusplot(customer[, c("grocery", "frozen")],
#          y_kmeans,
#          lines = 0,
#          shade = TRUE,
#          color = TRUE,
#          labels = 2,
#          plotchar = FALSE,
#          span = TRUE,
#          main = paste("Cluster"),
#          xlab = 'grocery',
#          ylab = 'frozen')

```





# The end

> How is this 'machine learning'? Just feels like fairly standard cluster analysis.

```{r runToHere}
```

